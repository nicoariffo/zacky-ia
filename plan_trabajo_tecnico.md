# PLAN DE TRABAJO T√âCNICO
## MVP ‚Äî Asistente IA para Soporte (Zendesk)

**Cliente:** E-commerce / Retail
**Sprint:** 4 semanas | 1 desarrollador full-time
**Fecha:** Febrero 2026 ‚Äî v2.0 (supuestos resueltos)

---

## 1. Contexto y Supuestos Resueltos

Este plan de trabajo est√° construido sobre decisiones concretas, no supuestos. A continuaci√≥n se documenta cada variable resuelta que condiciona la arquitectura, el alcance y los tiempos.

### 1.1 Variables Confirmadas

| Variable | Decisi√≥n | Impacto en el plan |
|----------|----------|--------------------|
| Fuente de datos | API de Zendesk (key disponible) | Se construye ingesta directa via API, no parser CSV. Backfill + incremental |
| Volumen de tickets | 5,000 a 50,000 tickets | Cabe en memoria para clustering. No se requiere infra distribuida (Spark). Embeddings procesables en horas, no d√≠as |
| Idioma | Solo espa√±ol | Sin pipeline multiling√ºe. Embeddings y prompts optimizados para espa√±ol. Sin detecci√≥n de idioma |
| Infraestructura | GCP (proyecto existente) | BigQuery + Cloud Run + Cloud Storage. Sin setup de cuenta. Terraform directo |
| Equipo | 1 persona full-time | Plan secuencial estricto. Sin paralelizaci√≥n. Cada fase debe terminar antes de iniciar la siguiente |
| Deadline | 4 semanas | Scope agresivo. Se priorizan entregables de mayor valor. Se recortan nice-to-haves |
| Proveedor IA | OpenAI (key disponible) | text-embedding-3-small para embeddings. GPT-4o-mini para generaci√≥n (costo/velocidad). GPT-4o para calidad si el budget lo permite |
| Destinatario | Cliente espec√≠fico (e-commerce) | Prompts y pol√≠ticas adaptados a retail: pedidos, env√≠os, devoluciones, pagos |
| Canales | Email + RRSS | Dos pipelines de limpieza distintos (email tiene firmas/quoted replies, RRSS es m√°s corto y con jerga) |
| Tags existentes | S√≠, pero inconsistentes | Tags como se√±al complementaria, no como fuente de verdad. El clustering sem√°ntico es el approach principal |
| UI esperada | Streamlit / dashboard simple | Sin React custom. Streamlit reduce tiempo de UI de 2 semanas a 3-4 d√≠as |
| Validaci√≥n humana | 1-2 agentes del cliente | Se necesita coordinar sesi√≥n de etiquetado (semana 2) y validaci√≥n de sugerencias (semana 3-4) |

> ‚ö†Ô∏è **Restricci√≥n cr√≠tica: 4 semanas / 1 persona**
> Este timeline requiere disciplina extrema en scope. Se definir√°n cortes expl√≠citos en cada fase. Lo que no entre en 4 semanas pasa a backlog post-MVP, no se negocia durante el sprint.

---

## 2. Arquitectura T√©cnica Definitiva

### 2.1 Stack confirmado

| Capa | Tecnolog√≠a | Justificaci√≥n |
|------|------------|---------------|
| Lenguaje | Python 3.11+ | Ecosistema ML, FastAPI, Streamlit. Todo en un lenguaje |
| API | FastAPI | Async, tipado, docs auto-generadas. Liviano para 1 persona |
| Storage principal | BigQuery | Ya disponible en GCP. SQL para an√°lisis. Sin administrar DB |
| Storage metadata | Cloud SQL (PostgreSQL) o Firestore | Para feedback, sesiones, estado de jobs. Evaluar seg√∫n complejidad |
| Embeddings | OpenAI text-embedding-3-small | 1536 dims, $0.02/1M tokens. Para 50K tickets ~$2-5 total |
| Generaci√≥n | GPT-4o-mini (default) + GPT-4o (calidad) | Mini para iteraci√≥n r√°pida ($0.15/1M). 4o para producci√≥n ($2.50/1M) |
| Clustering | HDBSCAN + UMAP | No requiere definir K. Maneja ruido. UMAP mejora calidad |
| UI | Streamlit | Prototipado r√°pido, componentes built-in, deploy en Cloud Run |
| Hosting | Cloud Run | Serverless, auto-scale, sin ops. API + Streamlit como servicios separados |
| CI/CD | GitHub Actions | Build, test, deploy a Cloud Run autom√°tico |
| Observabilidad | Cloud Logging + BigQuery logs | Sin herramientas extra. Queries sobre logs en BQ |

### 2.2 Esquema de datos (BigQuery)

| Tabla | Campos principales | Fase |
|-------|-------------------|------|
| `raw_tickets` | ticket_id, subject, description, comments_json, created_at, updated_at, tags, channel, assignee, status, priority, requester_email | Fase 1 |
| `clean_tickets` | ticket_id, text_full, text_customer_only, text_agent_only, channel, word_count, has_pii_redacted | Fase 2 |
| `embeddings` | ticket_id, embedding_vector (FLOAT64 REPEATED), model_version, created_at | Fase 3 |
| `clusters` | ticket_id, cluster_id, distance_to_centroid, is_noise, umap_x, umap_y | Fase 3 |
| `intents` | intent_id, cluster_id, name, description, volume, avg_resolution_time, repetition_score, risk_level, composite_score, status (active/inactive) | Fase 3-4 |
| `suggestions` | suggestion_id, ticket_id, intent_id, response_text, confidence_score, similar_ticket_ids, prompt_version, created_at | Fase 5 |
| `feedback` | feedback_id, suggestion_id, ticket_id, agent_id, action (accept/edit/reject), edited_text, rejection_reason, created_at | Fase 6 |

### 2.3 Estructura del repositorio

| Directorio | Contenido |
|------------|-----------|
| `src/ingestion/` | zendesk_client.py, backfill.py, incremental.py |
| `src/processing/` | cleaner.py, pii_redactor.py, pipeline.py |
| `src/intents/` | embeddings.py, clustering.py, labeling.py, scoring.py |
| `src/generation/` | response_generator.py, prompt_manager.py, confidence.py |
| `src/api/` | main.py, routes/, models/, middleware/ |
| `src/ui/` | app.py (Streamlit), pages/ (multi-page) |
| `prompts/` | system.txt, intents/*.yaml (template + pol√≠tica por intent) |
| `infra/` | terraform/, Dockerfile.api, Dockerfile.ui, docker-compose.yml |
| `tests/` | test_ingestion/, test_processing/, test_intents/, test_api/ |
| `notebooks/` | 01_eda.ipynb, 02_clustering_exploration.ipynb |
| `scripts/` | setup_bq.py, seed_data.py, run_backfill.sh |

---

## 3. Plan Semanal Detallado

El plan est√° organizado en 4 semanas de 5 d√≠as h√°biles cada una (40 horas/semana). Cada d√≠a tiene un entregable verificable.

---

### Semana 1 ‚Äî Fundamentos: Setup + Ingesta + Limpieza

**Objetivo:** Tener tickets limpios, sin PII, listos para generar embeddings. Al final de esta semana debes poder ejecutar un query en BigQuery que retorne tickets limpios.

| D√≠a | Bloque | Tareas espec√≠ficas | Entregable verificable |
|-----|--------|--------------------|----------------------|
| L (1) | Setup proyecto | Crear repo, estructura de carpetas, pyproject.toml, Docker base, docker-compose con PG local. Terraform: BQ datasets (raw, clean, features), Cloud Storage bucket, service accounts | Repo con estructura completa. `make setup` funciona. BQ datasets creados |
| M (2) | Ingesta Zendesk | Implementar zendesk_client.py: auth, paginaci√≥n cursor-based, rate limiting con backoff. Job de backfill con checkpoint. Escribir a raw_tickets en BQ | Backfill ejecutado: N tickets en raw_tickets. Log de progreso visible |
| X (3) | Ingesta + limpieza b√°sica | AM: Job incremental + tests de ingesta. PM: Inicio de cleaner.py: consolidar texto, separar mensajes cliente/agente, eliminar HTML, URLs de tracking, quoted replies | Incremental corriendo. Primeros 100 tickets limpios revisados manualmente |
| J (4) | Limpieza completa | Limpieza de firmas de email (regex + heur√≠sticas), templates de respuesta autom√°tica. Pipeline de limpieza espec√≠fico para RRSS (texto m√°s corto, emojis, menciones) | Pipeline limpieza ejecutado sobre dataset completo. clean_tickets poblada |
| V (5) | PII + validaci√≥n | Implementar pii_redactor.py: regex para emails, tel√©fonos (+56 X XXXX XXXX), RUTs. Validaci√≥n: muestra de 100 tickets revisada, reporte de calidad | QA report: % PII detectada, % falsos positivos. Dataset limpio validado |

**Decisiones t√©cnicas Semana 1:**
- Checkpoint de backfill: guardar cursor en archivo local. Si falla, retomar desde √∫ltimo cursor
- Limpieza de firmas: empezar con regex (l√≠neas que empiezan con `--`, `Enviado desde`, etc.). No usar ML para esto en MVP
- PII: solo regex en MVP. spaCy NER queda como enhancement post-MVP (agrega complejidad y dependencia pesada)
- Canal RRSS: normalizar menciones (@usuario) y hashtags. No eliminar emojis (pueden ser se√±al de sentiment)

---

### Semana 2 ‚Äî Core: Embeddings + Clustering + Intents

**Objetivo:** Tener un cat√°logo de intents etiquetados y priorizados. Esta es la semana m√°s cr√≠tica del proyecto.

> üî¥ **Hito clave: sesi√≥n de etiquetado con el cliente**
> El jueves o viernes de esta semana se necesita una sesi√≥n de 1-2 horas con alguien del equipo de soporte del cliente para validar y nombrar los clusters. Coordinar esto al inicio de la semana.

| D√≠a | Bloque | Tareas espec√≠ficas | Entregable verificable |
|-----|--------|--------------------|----------------------|
| L (6) | Embeddings | Implementar embeddings.py: batch processing con OpenAI API, rate limiting, escritura a BQ. Procesar dataset completo. Para 50K tickets: ~2-4 horas de procesamiento | Tabla embeddings poblada. Costo de API registrado |
| M (7) | UMAP + HDBSCAN | Reducci√≥n dimensional con UMAP (1536 ‚Üí 25 dims). Clustering con HDBSCAN. Experimentar con min_cluster_size (3-5% del dataset). Generar visualizaci√≥n 2D | Clusters generados. Visualizaci√≥n 2D de clusters. M√©tricas: N clusters, % noise, silhouette score |
| X (8) | An√°lisis de clusters | Para cada cluster: extraer 5 tickets representativos (m√°s cercanos al centroide). Generar resumen autom√°tico con GPT-4o-mini. Calcular m√©tricas: tama√±o, cohesi√≥n, overlap con tags existentes | Reporte de clusters: top 15-20 con resumen, tama√±o, ejemplos. Listo para revisi√≥n humana |
| J (9) | Etiquetado + scoring | Sesi√≥n con cliente para etiquetar clusters. Implementar scoring: volumen, tiempo resoluci√≥n, repetici√≥n sem√°ntica, riesgo (input del cliente). F√≥rmula de score compuesto | Cat√°logo de intents nombrados. Top 3-5 intents priorizados con scores |
| V (10) | Consolidaci√≥n + tests | Persistir intents en BQ. Mapping ticket-intent. Tests de estabilidad del clustering. Documentar decisiones de par√°metros. Preparar para Semana 3 | Tablas intents y clusters finales. Notebook de documentaci√≥n |

**Decisiones t√©cnicas Semana 2:**
- Embeddings: usar text-embedding-3-small (no ada-002). Mejor calidad, mismo precio. Embedding sobre `text_customer_only` (solo mensajes del cliente, no respuestas del agente)
- UMAP params: n_neighbors=15, min_dist=0.1, n_components=25 para clustering (no 2D, eso es solo para visualizaci√≥n)
- HDBSCAN: min_cluster_size = max(20, 3% del dataset). min_samples = 5. Esto evita micro-clusters
- Si hay >20 clusters: agrupar manualmente los que representan el mismo intent con formulaci√≥n diferente
- Tags de Zendesk: usar como validaci√≥n cruzada, no como input. Si un cluster se alinea con un tag existente, es buena se√±al

---

### Semana 3 ‚Äî Generaci√≥n IA + API

**Objetivo:** Tener la API funcionando y generando respuestas sugeridas con score de confianza para los intents priorizados.

| D√≠a | Bloque | Tareas espec√≠ficas | Entregable verificable |
|-----|--------|--------------------|----------------------|
| L (11) | Prompt engineering | Dise√±ar prompt templates para top 3-5 intents. System prompt con tono de marca. Pol√≠tica por intent (qu√© puede/no puede decir). Few-shot examples de tickets reales del cluster | Archivos `prompts/intents/*.yaml` con template + pol√≠tica + examples |
| M (12) | Motor de generaci√≥n | response_generator.py: detectar intent (distancia al centroide m√°s cercano), seleccionar prompt, inyectar contexto, llamar API, parsear respuesta. Confidence score basado en distancia al centroide | Script que dado un ticket_id retorna: intent, respuesta, confianza. Probado con 10 tickets |
| X (13) | API Core | FastAPI: GET /tickets, GET /tickets/{id}/suggestion, POST /tickets/{id}/feedback, GET /intents, GET /metrics/summary. Auth con API key. CORS. Deploy a Cloud Run staging | API en Cloud Run con docs Swagger accesibles. Endpoints probados con curl |
| J (14) | Evaluaci√≥n + tuning | Evaluar calidad de respuestas sobre 30-50 tickets por intent. Iterar prompts. Ajustar confidence thresholds. Agregar justificaci√≥n (tickets similares) | Reporte de evaluaci√≥n: % respuestas aceptables por intent. Prompts v2 mejorados |
| V (15) | Cache + robustez | Cachear sugerencias ya generadas (no regenerar). Manejo de errores (API down, ticket sin intent claro, confianza muy baja). Tests de API. Variables din√°micas (placeholders) | API robusta con error handling. Tests pasando. Cache funcional |

**Decisiones t√©cnicas Semana 3:**
- Detecci√≥n de intent en producci√≥n: calcular embedding del ticket nuevo, encontrar centroide m√°s cercano. Si distancia > threshold, clasificar como `no_intent` (sin sugerencia)
- Confidence score: normalizar distancia al centroide a escala 0-1. Umbral sugerido: >0.75 alta, 0.5-0.75 media, <0.5 no sugerir
- Prompt structure: system (tono/marca) + intent_policy (restricciones) + few_shot (3 ejemplos) + ticket_context (texto limpio)
- GPT-4o-mini para generaci√≥n en producci√≥n (costo). GPT-4o solo si la calidad de mini no es suficiente en evaluaci√≥n
- No auto-env√≠o: la API solo retorna sugerencias, nunca escribe en Zendesk autom√°ticamente

---

### Semana 4 ‚Äî UI + Dashboard + Validaci√≥n

**Objetivo:** Entregar el MVP completo: interfaz usable por agentes, dashboard de m√©tricas, y validaci√≥n con usuarios reales.

> üî¥ **Hito clave: validaci√≥n con agentes reales**
> Jueves o viernes: sesi√≥n con 1-2 agentes del cliente usando el sistema con tickets reales. Esto genera los primeros datos de feedback y valida la usabilidad.

| D√≠a | Bloque | Tareas espec√≠ficas | Entregable verificable |
|-----|--------|--------------------|----------------------|
| L (16) | Streamlit - Vista tickets | Page 1: lista de tickets con filtros (canal, fecha, intent, con/sin sugerencia). Vista de detalle: conversaci√≥n completa, intent detectado, sugerencia con confianza | App Streamlit corriendo local con datos reales |
| M (17) | Streamlit - HITL | Componentes de feedback: botones aceptar/editar/rechazar. Modal de edici√≥n. Raz√≥n de rechazo. Todo escribe a tabla feedback via API. Panel de confianza visual (verde/amarillo/rojo) | Flujo completo: ver ticket ‚Üí ver sugerencia ‚Üí dar feedback. Datos en BQ |
| X (18) | Dashboard m√©tricas | Page 2: KPIs cards (tickets con sugerencia, tasa aceptaci√≥n, horas ahorradas). Gr√°ficos: distribuci√≥n por intent, confianza promedio, timeline de feedback. Tabla de intents con m√©tricas | Dashboard con datos reales (o simulados si no hay feedback a√∫n) |
| J (19) | Deploy + validaci√≥n | Deploy Streamlit a Cloud Run. Configurar acceso para agentes del cliente. Sesi√≥n de validaci√≥n: agentes procesan 20-30 tickets reales con el sistema. Recoger feedback | Sistema en producci√≥n. Primeros datos de feedback reales |
| V (20) | Ajustes + entrega | Fixes de la sesi√≥n de validaci√≥n. Ajustar prompts si hay patrones de rechazo. Documentaci√≥n de uso. Exportar primeras m√©tricas. Preparar presentaci√≥n de resultados | MVP entregado. Documentaci√≥n. M√©tricas iniciales. Backlog post-MVP |

**Decisiones t√©cnicas Semana 4:**
- Streamlit multi-page: app.py como entry point, `pages/` con `1_Tickets.py`, `2_Dashboard.py`, `3_Intents.py`
- Feedback loop: cada acci√≥n del agente se guarda con timestamp, agent_id, y contexto. Esto permite calcular m√©tricas reales desde d√≠a 1
- Horas ahorradas = tickets aceptados √ó tiempo_promedio_resoluci√≥n_del_intent. Es una estimaci√≥n, pero da un n√∫mero concreto al cliente
- Deploy: Cloud Run con 2 servicios: api (FastAPI) + ui (Streamlit). Streamlit llama a la API internamente
- Acceso: Streamlit con autenticaci√≥n b√°sica (usuario/contrase√±a) o IAP de GCP si el cliente lo tiene

---

## 4. Cronograma Consolidado

| Semana | Foco principal | D√≠as | Entregable de cierre |
|--------|---------------|------|---------------------|
| Semana 1 | Setup + Ingesta + Limpieza + PII | 5 d√≠as | Dataset limpio en BigQuery, validado, sin PII |
| Semana 2 | Embeddings + Clustering + Intents + Scoring | 5 d√≠as | Cat√°logo de 3-5 intents priorizados y etiquetados |
| Semana 3 | Generaci√≥n IA + API + Evaluaci√≥n | 5 d√≠as | API en Cloud Run generando sugerencias con confianza |
| Semana 4 | Streamlit UI + Dashboard + Validaci√≥n | 5 d√≠as | MVP completo desplegado y validado con agentes reales |

### 4.1 Dependencias entre semanas

- Semana 2 depende de: dataset limpio de Semana 1
- Semana 3 depende de: cat√°logo de intents de Semana 2
- Semana 4 depende de: API funcional de Semana 3
- **Dependencia externa:** sesi√≥n de etiquetado con cliente (coordinar en Semana 1 para ejecutar en Semana 2)
- **Dependencia externa:** acceso de agentes al sistema (coordinar en Semana 3 para validaci√≥n en Semana 4)

---

## 5. M√©tricas de √âxito del MVP

### 5.1 M√©tricas t√©cnicas (controlables)

| M√©trica | Target | C√≥mo se mide |
|---------|--------|--------------|
| Tickets ingestados sin error | 100% | Count en raw_tickets vs total en Zendesk |
| Cobertura de limpieza | 100% de raw procesados | Count clean_tickets / count raw_tickets |
| PII redactada | <1% falsos negativos | QA manual sobre muestra de 100 tickets |
| Intents identificados | 3-5 intents activos | Count en tabla intents con status=active |
| Cobertura de intents | >40% del volumen total | Sum volumen de intents activos / total tickets |
| API response time | <3 segundos por sugerencia | Logs de API (p95 latency) |
| Uptime del sistema | >99% en semana de validaci√≥n | Cloud Run health checks |

### 5.2 M√©tricas de negocio (validaci√≥n con cliente)

| M√©trica | Target | C√≥mo se mide |
|---------|--------|--------------|
| Tickets con sugerencia IA | >40% en intents seleccionados | Count sugerencias generadas / tickets en intents activos |
| Tasa de aceptaci√≥n | >60% (aceptar + editar m√≠nimo) | Count (accept + edit) / total feedback |
| Tasa de rechazo | <40% | Count reject / total feedback |
| Horas ahorradas estimadas | C√°lculo visible en dashboard | Tickets aceptados √ó tiempo_promedio_resoluci√≥n |
| Feedback del equipo | Positivo en sesi√≥n de validaci√≥n | Cualitativo: encuesta post-sesi√≥n |

---

## 6. Riesgos y Plan de Mitigaci√≥n

| Riesgo | Prob. | Impacto | Mitigaci√≥n | Plan B |
|--------|-------|---------|------------|--------|
| Datos muy sucios | Alta | Alto | QA manual d√≠a 5. Si >30% inutilizable, recortar a subset limpio | Trabajar solo con tickets de email (suelen ser m√°s limpios) |
| Clustering no produce intents claros | Media | Alto | Probar 3+ configs de HDBSCAN. Reducir min_cluster_size. Visualizar manualmente | Usar tags existentes como seed + clustering como refinamiento |
| Cliente no disponible para etiquetado | Media | Alto | Coordinar desde d√≠a 1. Tener fecha bloqueada para semana 2 | Auto-etiquetar con LLM y validar asincr√≥nicamente por email |
| Respuestas IA de baja calidad | Media | Medio | Few-shot examples. Prompt tuning d√≠a 14. Pol√≠ticas estrictas por intent | Reducir a 2-3 intents simples y descartar los m√°s complejos |
| Rate limits OpenAI | Baja | Medio | Batch processing. Cache agresivo. Backoff exponencial | Reducir batch size. Procesar en horarios de baja demanda |
| No alcanza el tiempo | Alta | Alto | Cortar features no-core cada viernes. Priorizar flujo completo sobre perfecci√≥n | Entregar semana 3 sin UI bonita (demo via API + notebook) |

---

## 7. Qu√© Queda Fuera del MVP (Backlog Post-MVP)

Es tan importante definir qu√© no se hace como qu√© s√≠. Estos items quedan expl√≠citamente fuera de las 4 semanas:

| Feature | Raz√≥n de exclusi√≥n | Prioridad post-MVP |
|---------|-------------------|-------------------|
| Auto-reply (env√≠o autom√°tico) | Requiere confianza validada + aprobaci√≥n del cliente. Riesgo reputacional | Alta ‚Äî siguiente iteraci√≥n |
| Integraci√≥n con estados de pedido | Requiere acceso a ERP/OMS del cliente. Scope de integraci√≥n aparte | Alta ‚Äî habilita variables din√°micas |
| Detecci√≥n de PII con ML (spaCy NER) | Regex cubre 80%+ de casos. NER agrega dependencia pesada y complejidad | Media ‚Äî enhancement |
| Multicanal (WhatsApp, chat en vivo) | MVP cubre email + RRSS. Otros canales requieren conectores adicionales | Media ‚Äî roadmap |
| Optimizaci√≥n por CSAT | Requiere datos de satisfacci√≥n + correlaci√≥n. No hay suficiente feedback en MVP | Media ‚Äî post validaci√≥n |
| Multi-tenant (m√∫ltiples clientes) | Este MVP es para un cliente. Arquitectura multi-tenant es otro proyecto | Alta ‚Äî si se convierte en producto |
| React UI custom | Streamlit cubre la necesidad del MVP. React solo si se necesita UX avanzada | Baja ‚Äî solo si Streamlit limita |
| CI/CD completo con staging | Se deployar√° directo a producci√≥n con feature flags. Pipeline b√°sico | Media ‚Äî profesionalizar post-MVP |
| Modelo de pricing por impacto | Primero validar que funciona. Pricing viene despu√©s | Alta ‚Äî comercial |
| Re-entrenamiento autom√°tico | El feedback se acumula pero el re-clustering es manual en MVP | Media ‚Äî automatizar post-MVP |

---

## 8. Costos Estimados de Infraestructura y APIs

Estimaci√≥n para 4 semanas de desarrollo + operaci√≥n inicial con ~30,000 tickets:

| Concepto | Estimaci√≥n | Notas |
|----------|------------|-------|
| OpenAI ‚Äî Embeddings | $2 - $5 USD | text-embedding-3-small. ~30K tickets √ó ~500 tokens promedio |
| OpenAI ‚Äî Generaci√≥n | $10 - $30 USD | GPT-4o-mini para desarrollo. ~1000 generaciones de prueba + producci√≥n |
| BigQuery | $0 - $5 USD | Primer TB de queries gratis. Dataset peque√±o. Storage m√≠nimo |
| Cloud Run | $5 - $20 USD | 2 servicios (API + UI). Tr√°fico bajo en MVP. Scale to zero |
| Cloud Storage | <$1 USD | Solo para artefactos y backups |
| **Total estimado (4 semanas)** | **$20 - $60 USD** | Sin contar horas de desarrollo |

---

## 9. Checklist Pre-Arranque (D√≠a 0)

Antes de escribir la primera l√≠nea de c√≥digo, confirmar que todo esto est√° resuelto:

| Item | Estado | Responsable | Notas |
|------|--------|-------------|-------|
| API key de Zendesk con permisos de lectura | Pendiente | T√∫ / Cliente | Necesita read access a tickets, users, comments |
| Proyecto GCP con billing activo | Confirmado | T√∫ | Verificar que BQ, Cloud Run, Cloud Storage est√°n habilitados |
| API key de OpenAI con cr√©ditos | Confirmado | T√∫ | Verificar rate limits del plan actual |
| Repositorio Git creado | Pendiente | T√∫ | GitHub o GitLab. Definir branching strategy |
| Fecha de sesi√≥n de etiquetado (semana 2) | Pendiente | T√∫ + Cliente | Bloquear 2 horas en agenda del equipo de soporte |
| Fecha de validaci√≥n con agentes (semana 4) | Pendiente | T√∫ + Cliente | Bloquear 2-3 horas con 1-2 agentes |
| Acceso a datos de ejemplo (5-10 tickets) | Pendiente | Cliente | Para validar formato antes del backfill masivo |
| Contacto t√©cnico del cliente | Pendiente | T√∫ | Qui√©n responde dudas sobre datos, procesos, tono de marca |

---

## 10. Pr√≥ximos Pasos

Con los supuestos resueltos, el camino es claro:

- Resolver todos los items del checklist de D√≠a 0
- Iniciar Semana 1, D√≠a 1: setup del repositorio e infraestructura
- Coordinar con el cliente la sesi√≥n de etiquetado para Semana 2
- Ejecutar el plan d√≠a a d√≠a, cortando scope si es necesario cada viernes

**Criterio de corte semanal:** cada viernes, evaluar si el entregable de cierre de semana se cumpli√≥. Si no, recortar el scope de la semana siguiente para compensar. Nunca acumular deuda. Lo que no entra, va al backlog post-MVP.
